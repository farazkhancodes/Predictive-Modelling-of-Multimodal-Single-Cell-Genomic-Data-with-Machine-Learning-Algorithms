{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FEATURE ENGINEERING AND DATA PREPARATION** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T13:19:39.759875Z",
     "iopub.status.busy": "2023-11-01T13:19:39.759442Z",
     "iopub.status.idle": "2023-11-01T13:19:40.211515Z",
     "shell.execute_reply": "2023-11-01T13:19:40.210271Z",
     "shell.execute_reply.started": "2023-11-01T13:19:39.759834Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action = 'ignore')\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T13:19:40.215457Z",
     "iopub.status.busy": "2023-11-01T13:19:40.214178Z",
     "iopub.status.idle": "2023-11-01T13:19:41.742134Z",
     "shell.execute_reply": "2023-11-01T13:19:41.740812Z",
     "shell.execute_reply.started": "2023-11-01T13:19:40.215403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>donor</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cell_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c2150f55becb</th>\n",
       "      <td>2</td>\n",
       "      <td>27678</td>\n",
       "      <td>HSC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65b7edf8a4da</th>\n",
       "      <td>2</td>\n",
       "      <td>27678</td>\n",
       "      <td>HSC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c1b26cb1057b</th>\n",
       "      <td>2</td>\n",
       "      <td>27678</td>\n",
       "      <td>EryP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917168fa6f83</th>\n",
       "      <td>2</td>\n",
       "      <td>27678</td>\n",
       "      <td>NeuP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2b29feeca86d</th>\n",
       "      <td>2</td>\n",
       "      <td>27678</td>\n",
       "      <td>EryP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a9b4d99f1f50</th>\n",
       "      <td>7</td>\n",
       "      <td>31800</td>\n",
       "      <td>HSC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0e2c1d0782af</th>\n",
       "      <td>7</td>\n",
       "      <td>31800</td>\n",
       "      <td>HSC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a3cbc5aa0ec3</th>\n",
       "      <td>7</td>\n",
       "      <td>31800</td>\n",
       "      <td>MkP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75b350243add</th>\n",
       "      <td>7</td>\n",
       "      <td>31800</td>\n",
       "      <td>EryP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ad5a949989b2</th>\n",
       "      <td>7</td>\n",
       "      <td>31800</td>\n",
       "      <td>EryP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119651 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              day  donor cell_type  gender\n",
       "cell_id                                   \n",
       "c2150f55becb    2  27678       HSC       1\n",
       "65b7edf8a4da    2  27678       HSC       1\n",
       "c1b26cb1057b    2  27678      EryP       1\n",
       "917168fa6f83    2  27678      NeuP       1\n",
       "2b29feeca86d    2  27678      EryP       1\n",
       "...           ...    ...       ...     ...\n",
       "a9b4d99f1f50    7  31800       HSC       1\n",
       "0e2c1d0782af    7  31800       HSC       1\n",
       "a3cbc5aa0ec3    7  31800       MkP       1\n",
       "75b350243add    7  31800      EryP       1\n",
       "ad5a949989b2    7  31800      EryP       1\n",
       "\n",
       "[119651 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = pd.read_hdf(\"./Data/train_cite_targets.h5\")\n",
    "meta_df = pd.read_csv('./Data/metadata.csv', index_col = 'cell_id')\n",
    "meta_df = meta_df[meta_df.technology == 'citeseq'].drop(columns = 'technology')\n",
    "meta_df['gender'] = meta_df.donor.apply(lambda x : 1 if x != 13176 else 0)\n",
    "meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T13:19:41.744127Z",
     "iopub.status.busy": "2023-11-01T13:19:41.743735Z",
     "iopub.status.idle": "2023-11-01T13:19:41.755072Z",
     "shell.execute_reply": "2023-11-01T13:19:41.753777Z",
     "shell.execute_reply.started": "2023-11-01T13:19:41.744095Z"
    }
   },
   "outputs": [],
   "source": [
    "def fix_test():        # Test Data Had Some Issue Which Was Later Fixed By The Publisher\n",
    "    global test, meta_df\n",
    "    extra_columns = ['ENSG00000255655_AC007570.1','ENSG00000213237_AC008155.1','ENSG00000273398_AC017083.3','ENSG00000233849_AC022201.2','ENSG00000231401_AC023481.1','ENSG00000263126_AC040162.3','ENSG00000285876_AC093912.1','ENSG00000275585_AC241377.2','ENSG00000272175_AL050343.2','ENSG00000232290_AL133260.2','ENSG00000267559_AL158154.2','ENSG00000273481_AL391069.4','ENSG00000161609_CCDC155','ENSG00000150394_CDH8','ENSG00000233932_CTXN2','ENSG00000198077_CYP2A7','ENSG00000220871_DNAJC19P6','ENSG00000267336_EIF4A2P1','ENSG00000251254_GTF2F2P1','ENSG00000225978_HAR1A','ENSG00000131097_HIGD1B','ENSG00000211897_IGHG3','ENSG00000233080_LINC01399','ENSG00000236849_LINC01474','ENSG00000269904_MAP2K4P1','ENSG00000266698_MIR3945','ENSG00000144227_NXPH2','ENSG00000182057_OGFRP1','ENSG00000251107_PDCD5P2','ENSG00000226964_RHEBP2','ENSG00000214988_RPL7AP26','ENSG00000248873_SERBP1P6','ENSG00000212588_SNORA26','ENSG00000166984_TCP10L2','ENSG00000226051_ZNF503-AS1']\n",
    "    meta_fix = pd.read_csv(\"./Data/metadata_cite_day_2_donor_27678.csv\", index_col = 'cell_id')\n",
    "    test_fix = pd.read_hdf(\"./Data/test_cite_inputs_day_2_donor_27678.h5\")\n",
    "    test_fix.drop(columns = extra_columns, inplace = True)\n",
    "    filter_out = meta_df[(meta_df.day == 2) & (meta_df.donor == 27678)].index\n",
    "    test.drop(index = filter_out, inplace = True)\n",
    "    test = pd.concat([test, test_fix], axis = 0)\n",
    "    meta_df.drop(index = filter_out, inplace = True)\n",
    "    meta_df = pd.concat([meta_df, meta_fix], axis = 0)\n",
    "    \n",
    "    del extra_columns, meta_fix, test_fix, filter_out\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T13:19:41.758502Z",
     "iopub.status.busy": "2023-11-01T13:19:41.758089Z",
     "iopub.status.idle": "2023-11-01T13:20:39.382426Z",
     "shell.execute_reply": "2023-11-01T13:20:39.381351Z",
     "shell.execute_reply.started": "2023-11-01T13:19:41.758469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449 Null Columns and 37 Important Columns\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_hdf(\"./Data/train_cite_inputs.h5\")\n",
    "meta_train = meta_df.reindex(train.index)\n",
    "train = pd.concat([train, meta_train[['day', 'donor', 'gender']]], axis = 1)\n",
    "\n",
    "# Remove Useless Data And Adding Important Columns\n",
    "notimp_cols = list(train.columns[(train == 0).all().values])\n",
    "imp_cols = [_ for _ in train.columns if _[_.find('_') + 1 : ] in targets.columns] + ['day', 'donor', 'gender']\n",
    "print(f\"{len(notimp_cols)} Null Columns and {len(imp_cols)} Important Columns\")\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD as tSVD\n",
    "def transform(data, dimension):\n",
    "    global notimp_cols, imp_cols\n",
    "    print(\"Shape Of Data Before Processing \",data.shape)\n",
    "    data.drop(columns = notimp_cols, inplace = True)\n",
    "    print('- useless columns dropped')\n",
    "    \n",
    "    data_imp_cols = data[imp_cols].to_numpy()\n",
    "    data.drop(columns = ['day', 'donor', 'gender'], inplace = True)\n",
    "    \n",
    "    print('- important columns saved')\n",
    "    data = data - data.median() \n",
    "    data = data.apply(lambda row: row / row.std(), axis = 1)   \n",
    "    print('- standardized the data')\n",
    "    data = data.to_numpy()\n",
    "    # Reducing them to lower dimensions using PCA\n",
    "    pca = tSVD(n_components = dimension, random_state = 42)\n",
    "    data = pca.fit_transform(data)\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    print(f\"- transformed to {dimension} dimensions : explaining {str(sum(evr)*100)[:5]} % of data\")\n",
    "    data = np.hstack([data, data_imp_cols])\n",
    "    print('- stacked the important columns')\n",
    "    print(\"Shape Of Data After Removal \",data.shape)\n",
    "    print(\"-\"* 65)\n",
    "    print()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T13:20:39.384376Z",
     "iopub.status.busy": "2023-11-01T13:20:39.383997Z",
     "iopub.status.idle": "2023-11-01T13:27:50.473961Z",
     "shell.execute_reply": "2023-11-01T13:27:50.472604Z",
     "shell.execute_reply.started": "2023-11-01T13:20:39.384345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- For Train Data -------------------------\n",
      "Shape Of Data Before Processing  (70988, 22053)\n",
      "- useless columns dropped\n",
      "- important columns saved\n",
      "- standardized the data\n",
      "- transformed to 100 dimensions : explaining 12.90 % of data\n",
      "- stacked the important columns\n",
      "Shape Of Data After Removal  (70988, 137)\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "---------------------- For Donor Test Data ----------------------\n",
      "Shape Of Data Before Processing  (21336, 22053)\n",
      "- useless columns dropped\n",
      "- important columns saved\n",
      "- standardized the data\n",
      "- transformed to 100 dimensions : explaining 13.21 % of data\n",
      "- stacked the important columns\n",
      "Shape Of Data After Removal  (21336, 137)\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "---------------------- For Day Test Data ----------------------\n",
      "Shape Of Data Before Processing  (26867, 22053)\n",
      "- useless columns dropped\n",
      "- important columns saved\n",
      "- standardized the data\n",
      "- transformed to 100 dimensions : explaining 17.87 % of data\n",
      "- stacked the important columns\n",
      "Shape Of Data After Removal  (26867, 137)\n",
      "-----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_dim = 100\n",
    "print(\"-\"*25, \"For Train Data\", \"-\"*25)\n",
    "train = transform(train, new_dim)\n",
    "\n",
    "test = pd.read_hdf(\"./Data/test_cite_inputs.h5\")\n",
    "fix_test()\n",
    "meta_test = meta_df.reindex(test.index)\n",
    "test = pd.concat([test, meta_test[['day', 'donor', 'gender']]], axis = 1)\n",
    "\n",
    "print(\"-\"*22, \"For Donor Test Data\", \"-\"*22)\n",
    "meta_donor_test = meta_test[(meta_test.day != 7) & (meta_test.donor == 27678)]\n",
    "donor_test = test.reindex(meta_donor_test.index)\n",
    "donor_test = transform(donor_test, new_dim)\n",
    "\n",
    "print(\"-\"*22, \"For Day Test Data\", \"-\"*22)\n",
    "meta_day_test = meta_test[meta_test.day == 7]\n",
    "day_test = test.reindex(meta_day_test.index)\n",
    "del test     #useless data now, deleting to free up space\n",
    "gc.collect()\n",
    "day_test = transform(day_test, new_dim)\n",
    "\n",
    "targets = targets.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T13:27:50.475953Z",
     "iopub.status.busy": "2023-11-01T13:27:50.475566Z",
     "iopub.status.idle": "2023-11-01T13:27:50.579046Z",
     "shell.execute_reply": "2023-11-01T13:27:50.577733Z",
     "shell.execute_reply.started": "2023-11-01T13:27:50.475921Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del notimp_cols, imp_cols, meta_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T13:27:50.581498Z",
     "iopub.status.busy": "2023-11-01T13:27:50.581009Z",
     "iopub.status.idle": "2023-11-01T13:27:50.750250Z",
     "shell.execute_reply": "2023-11-01T13:27:50.749170Z",
     "shell.execute_reply.started": "2023-11-01T13:27:50.581453Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving The Clean Data\n",
    "np.save('train', train)\n",
    "np.save('donor_test', donor_test)\n",
    "np.save('day_test', day_test)\n",
    "np.save('targets', targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MODEL BUILDING** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T13:27:50.752112Z",
     "iopub.status.busy": "2023-11-01T13:27:50.751684Z",
     "iopub.status.idle": "2023-11-01T13:27:51.997116Z",
     "shell.execute_reply": "2023-11-01T13:27:51.996008Z",
     "shell.execute_reply.started": "2023-11-01T13:27:50.752080Z"
    }
   },
   "outputs": [],
   "source": [
    "# Trun On The GPU And Run This (Not required if running locally)\n",
    "import warnings\n",
    "warnings.simplefilter(action = 'ignore')\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import gc\n",
    "\n",
    "# Load The Data\n",
    "train = np.load(\"train.npy\")\n",
    "targets = np.load(\"targets.npy\")\n",
    "donor_test = np.load(\"train.npy\")\n",
    "day_test = np.load(\"train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T13:27:51.998809Z",
     "iopub.status.busy": "2023-11-01T13:27:51.998426Z",
     "iopub.status.idle": "2023-11-01T13:28:35.178913Z",
     "shell.execute_reply": "2023-11-01T13:28:35.177778Z",
     "shell.execute_reply.started": "2023-11-01T13:27:51.998756Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choosing features important for training each target column\n",
    "corr_col_dict = {}\n",
    "def corr_cols():\n",
    "    global corr_col_dict\n",
    "    for out in range(targets.shape[1]):\n",
    "        correlations = []\n",
    "        for inp in range(train.shape[1]):\n",
    "            correlations.append(np.corrcoef(train[:, inp], targets[:, out])[0, 1])\n",
    "        correlated_features = [correlations.index(i) for i in correlations if abs(i) > np.percentile(list(map(abs, correlations)), 75)]\n",
    "        corr_col_dict[out] = correlated_features\n",
    "corr_cols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T13:28:35.183699Z",
     "iopub.status.busy": "2023-11-01T13:28:35.182721Z",
     "iopub.status.idle": "2023-11-01T13:28:35.190557Z",
     "shell.execute_reply": "2023-11-01T13:28:35.189181Z",
     "shell.execute_reply.started": "2023-11-01T13:28:35.183661Z"
    }
   },
   "outputs": [],
   "source": [
    "def correlation_score(y_true, y_hat):\n",
    "    if type(y_true) == pd.DataFrame: y_true = y_true.values\n",
    "    if type(y_hat) == pd.DataFrame: y_hat = y_hat.values\n",
    "    corrsum = 0\n",
    "    for i in range(len(y_true)):\n",
    "        corrsum += np.corrcoef(y_true[i], y_hat[i])[1, 0]\n",
    "    return corrsum / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T13:28:35.193135Z",
     "iopub.status.busy": "2023-11-01T13:28:35.192756Z",
     "iopub.status.idle": "2023-11-01T13:28:56.954402Z",
     "shell.execute_reply": "2023-11-01T13:28:56.953002Z",
     "shell.execute_reply.started": "2023-11-01T13:28:35.193105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: lightgbm 4.1.0\n",
      "Uninstalling lightgbm-4.1.0:\n",
      "  Successfully uninstalled lightgbm-4.1.0\n",
      "Collecting lightgbm\n",
      "  Using cached lightgbm-4.1.0-py3-none-manylinux_2_28_x86_64.whl (3.1 MB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from lightgbm) (1.23.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from lightgbm) (1.11.2)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall lightgbm -y\n",
    "!pip install lightgbm --config-settings=cmake.define.USE_GPU=ON\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# Params For GroupKfold By Donor\n",
    "xgb_params = {\n",
    "    'n_estimators' : 300,\n",
    "    'tree_method' : 'gpu_hist',\n",
    "    'predictor' : 'gpu_predictor',\n",
    "    'gpu_id' : 0\n",
    "}\n",
    "lgbm_params = {\n",
    "     'n_estimators' : 300,\n",
    "     'learning_rate': 0.1, \n",
    "     'max_depth': 10, \n",
    "     'num_leaves': 200,\n",
    "     'min_child_samples': 250,\n",
    "     'colsample_bytree': 0.8, \n",
    "     'subsample': 0.6, \n",
    "     \"seed\" : 1,\n",
    "     \"device\" : \"gpu\",\n",
    "     \"verbose\" : -1\n",
    "}\n",
    "cat_params = {\n",
    "    \"iterations\" : 300,\n",
    "    \"depth\" : 6, \n",
    "    \"learning_rate\" : 0.1,\n",
    "    \"loss_function\" : \"RMSE\", \n",
    "    \"verbose\" : 0\n",
    "}\n",
    "\n",
    "# Initialize Individual Regressors\n",
    "xgb_regressor = XGBRegressor(**xgb_params)\n",
    "lgbm_regressor = LGBMRegressor(**lgbm_params)\n",
    "catboost_regressor = CatBoostRegressor(**cat_params)\n",
    "ensemble_regressor = VotingRegressor(estimators=[\n",
    "    ('xgb', xgb_regressor),\n",
    "    ('catboost', catboost_regressor),\n",
    "    ('lgbm', lgbm_regressor)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T13:28:56.956794Z",
     "iopub.status.busy": "2023-11-01T13:28:56.956302Z",
     "iopub.status.idle": "2023-11-01T14:24:36.925428Z",
     "shell.execute_reply": "2023-11-01T14:24:36.924226Z",
     "shell.execute_reply.started": "2023-11-01T13:28:56.956741Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 0 with Donors [13176, 32606] for Donor [31800] : |██████████|100%  140/140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0  137: mse = 2.39, corr =  0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1 with Donors [13176, 31800] for Donor [32606] : |██████████|100%  140/140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1  137: mse = 2.37, corr =  0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 2 with Donors [31800, 32606] for Donor [13176] : |██████████|100%  140/140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2  137: mse = 4.06, corr =  0.88\n",
      "\u001b[33mAverage MSE : 2.94, Average Correlation : 0.88\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# KFold Grouped By Donor\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from colorama import Fore, Style\n",
    "\n",
    "# for the purpose of fancy visuals\n",
    "from tqdm import tqdm as progress_bar\n",
    "import time\n",
    "\n",
    "y_cols = targets.shape[1]\n",
    "groups = train[ : , -2]\n",
    "\n",
    "kf = GroupKFold(n_splits = 3)\n",
    "score_list = []\n",
    "for fold, (idx_tr, idx_va) in enumerate(kf.split(train, groups = groups)):\n",
    "    X_tr = train[idx_tr]\n",
    "    y_tr = targets[:,: y_cols][idx_tr]\n",
    "    X_va = train[idx_va]\n",
    "    y_va = targets[:,: y_cols][idx_va]\n",
    "\n",
    "    pred_list = []\n",
    "    with progress_bar(total = y_cols, bar_format = '{desc} : |{bar}|{percentage:3.0f}%  {n_fmt}/{total_fmt}', desc = f'Training Fold {fold} with Donors {list(map(int, set(train[idx_tr][: , -2])))} for Donor {list(map(int, set(train[idx_va][: , -2])))}', unit = 'iteration') as pbar:\n",
    "        for i in range(y_cols):\n",
    "            X_tr_corr = X_tr[:, corr_col_dict[i]]\n",
    "            X_va_corr = X_va[:, corr_col_dict[i]]\n",
    "            \n",
    "            ensemble_regressor.fit(X_tr_corr, y_tr[:,i].copy())\n",
    "            pred_list.append(ensemble_regressor.predict(X_va_corr))\n",
    "                \n",
    "            time.sleep(0.1)\n",
    "            pbar.update(1)\n",
    "    y_hat = np.column_stack(pred_list) # concatenate the 140 predictions\n",
    "    del pred_list, X_tr, y_tr, X_tr_corr, X_va_corr\n",
    "    gc.collect()\n",
    "\n",
    "    # We validate the model (mse and correlation over all 140 columns)\n",
    "    mse = mean_squared_error(y_va, y_hat)\n",
    "    corrscore = correlation_score(y_va, y_hat)\n",
    "\n",
    "    del X_va, y_va\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"Fold {fold} {train.shape[1]:4}: mse = {mse:.2f}, corr =  {corrscore:.2f}\")\n",
    "    score_list.append((mse, corrscore))\n",
    "\n",
    "# Averaging Scores From All The Folds\n",
    "if len(score_list) > 1:\n",
    "    result_df = pd.DataFrame(score_list, columns=['mse', 'corrscore'])\n",
    "    print(Fore.YELLOW + f\"Average MSE : {result_df.mse.mean():.2f}, Average Correlation : {result_df.corrscore.mean():.2f}\"+ Style.RESET_ALL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T14:24:36.927914Z",
     "iopub.status.busy": "2023-11-01T14:24:36.927399Z",
     "iopub.status.idle": "2023-11-01T15:20:22.514313Z",
     "shell.execute_reply": "2023-11-01T15:20:22.513059Z",
     "shell.execute_reply.started": "2023-11-01T14:24:36.927869Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 0 with Days [2, 3] for Day [4] : |██████████|100%  140/140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0  137: mse = 2.64, corr =  0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1 with Days [3, 4] for Day [2] : |██████████|100%  140/140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1  137: mse = 2.85, corr =  0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 2 with Days [2, 4] for Day [3] : |██████████|100%  140/140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2  137: mse = 2.97, corr =  0.89\n",
      "\u001b[33mAverage MSE : 2.82, Average Correlation : 0.89\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# KFold Grouped By Day\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from colorama import Fore, Style\n",
    "\n",
    "# for the purpose of fancy visuals\n",
    "from tqdm import tqdm as progress_bar\n",
    "import time\n",
    "\n",
    "y_cols = targets.shape[1]\n",
    "groups = train[ : , -3]\n",
    "\n",
    "kf = GroupKFold(n_splits = 3)\n",
    "score_list = []\n",
    "for fold, (idx_tr, idx_va) in enumerate(kf.split(train, groups = groups)):\n",
    "    X_tr = train[idx_tr]\n",
    "    y_tr = targets[:,: y_cols][idx_tr]\n",
    "    X_va = train[idx_va]\n",
    "    y_va = targets[:,: y_cols][idx_va]\n",
    "\n",
    "    pred_list = []\n",
    "    with progress_bar(total = y_cols, bar_format = '{desc} : |{bar}|{percentage:3.0f}%  {n_fmt}/{total_fmt}', desc = f'Training Fold {fold} with Days {list(map(int, set(train[idx_tr][: , -3])))} for Day {list(map(int, set(train[idx_va][: , -3])))}', unit = 'iteration') as pbar:\n",
    "        for i in range(y_cols):\n",
    "            X_tr_corr = X_tr[:, corr_col_dict[i]]\n",
    "            X_va_corr = X_va[:, corr_col_dict[i]]\n",
    "            \n",
    "            ensemble_regressor.fit(X_tr_corr, y_tr[:,i].copy())\n",
    "            pred_list.append(ensemble_regressor.predict(X_va_corr))\n",
    "                \n",
    "            time.sleep(0.1)\n",
    "            pbar.update(1)\n",
    "    y_hat = np.column_stack(pred_list) # concatenate the 140 predictions\n",
    "    del pred_list, X_tr, y_tr, X_tr_corr, X_va_corr\n",
    "    gc.collect()\n",
    "\n",
    "    # We validate the model (mse and correlation over all 140 columns)\n",
    "    mse = mean_squared_error(y_va, y_hat)\n",
    "    corrscore = correlation_score(y_va, y_hat)\n",
    "\n",
    "    del X_va, y_va\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"Fold {fold} {train.shape[1]:4}: mse = {mse:.2f}, corr =  {corrscore:.2f}\")\n",
    "    score_list.append((mse, corrscore))\n",
    "\n",
    "# Averaging Scores From All The Folds\n",
    "if len(score_list) > 1:\n",
    "    result_df = pd.DataFrame(score_list, columns=['mse', 'corrscore'])\n",
    "    print(Fore.YELLOW + f\"Average MSE : {result_df.mse.mean():.2f}, Average Correlation : {result_df.corrscore.mean():.2f}\"+ Style.RESET_ALL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RETRAINING ON WHOLE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T15:31:42.924942Z",
     "iopub.status.busy": "2023-11-01T15:31:42.924565Z",
     "iopub.status.idle": "2023-11-01T15:57:07.135673Z",
     "shell.execute_reply": "2023-11-01T15:57:07.134245Z",
     "shell.execute_reply.started": "2023-11-01T15:31:42.924909Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retraining On Whole Data  : |██████████|100%  140/140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE On Whole Data is  1.50 and Correlation Score is  0.93\n"
     ]
    }
   ],
   "source": [
    "pred_list = []\n",
    "with progress_bar(total = y_cols, bar_format = '{desc} : |{bar}|{percentage:3.0f}%  {n_fmt}/{total_fmt}', desc = 'Retraining On Whole Data ', unit='iteration') as pbar:\n",
    "    for i in range(y_cols):\n",
    "        pt_corr = train[:, corr_col_dict[i]]\n",
    "        ensemble_regressor.fit(pt_corr, targets[:,i].copy())\n",
    "        pred_list.append(ensemble_regressor.predict(pt_corr))\n",
    "        time.sleep(0.1)\n",
    "        pbar.update(1)\n",
    "y_hat = np.column_stack(pred_list) # concatenate the 140 predictions\n",
    "mse = mean_squared_error(targets, y_hat)\n",
    "corrscore = correlation_score(targets, y_hat)\n",
    "print(f'MSE On Whole Data is {mse : .2f} and Correlation Score is {corrscore : .2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T15:57:07.138857Z",
     "iopub.status.busy": "2023-11-01T15:57:07.138296Z",
     "iopub.status.idle": "2023-11-01T16:04:42.854236Z",
     "shell.execute_reply": "2023-11-01T16:04:42.852918Z",
     "shell.execute_reply.started": "2023-11-01T15:57:07.138803Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing On Data With Donor 4  : |██████████|100%  140/140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.04630607 2.00784029 2.2451049  ... 1.65586986 2.40053819 1.19232374]\n",
      " [1.79529695 1.56196776 1.84762049 ... 1.43062198 1.87287806 1.28844676]\n",
      " [1.10319961 1.80775265 1.77694262 ... 1.70479396 1.82852748 2.08570162]\n",
      " ...\n",
      " [2.610289   2.77716355 2.3372659  ... 2.38049246 1.58823294 3.75252575]\n",
      " [2.35452687 2.98882973 4.14508958 ... 2.62446883 3.12422974 3.41985365]\n",
      " [2.09925628 3.12575942 2.24207375 ... 1.98744017 2.75329372 3.54321394]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing On Data From A Different Day  : |██████████|100%  140/140"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.04630607 2.00784029 2.2451049  ... 1.65586986 2.40053819 1.19232374]\n",
      " [1.79529695 1.56196776 1.84762049 ... 1.43062198 1.87287806 1.28844676]\n",
      " [1.10319961 1.80775265 1.77694262 ... 1.70479396 1.82852748 2.08570162]\n",
      " ...\n",
      " [2.610289   2.77716355 2.3372659  ... 2.38049246 1.58823294 3.75252575]\n",
      " [2.35452687 2.98882973 4.14508958 ... 2.62446883 3.12422974 3.41985365]\n",
      " [2.09925628 3.12575942 2.24207375 ... 1.98744017 2.75329372 3.54321394]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pred_list = []\n",
    "with progress_bar(total = y_cols, bar_format = '{desc} : |{bar}|{percentage:3.0f}%  {n_fmt}/{total_fmt}', desc = 'Testing On Data With Donor 4 ', unit='iteration') as pbar:\n",
    "    for i in range(y_cols):\n",
    "        pt_corr = donor_test[:, corr_col_dict[i]]\n",
    "        pred_list.append(ensemble_regressor.predict(pt_corr))\n",
    "        time.sleep(0.1)\n",
    "        pbar.update(1)\n",
    "y_hat = np.column_stack(pred_list) # concatenate the 140 predictions\n",
    "print(y_hat)\n",
    "\n",
    "pred_list = []\n",
    "with progress_bar(total = y_cols, bar_format = '{desc} : |{bar}|{percentage:3.0f}%  {n_fmt}/{total_fmt}', desc = 'Testing On Data From A Different Day ', unit='iteration') as pbar:\n",
    "    for i in range(y_cols):\n",
    "        pt_corr = day_test[:, corr_col_dict[i]]\n",
    "        pred_list.append(ensemble_regressor.predict(pt_corr))\n",
    "        time.sleep(0.1)\n",
    "        pbar.update(1)\n",
    "y_hat = np.column_stack(pred_list) # concatenate the 140 predictions\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
